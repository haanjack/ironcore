num_attention_heads: 16
num_attention_groups: 16
attention_head_size: 64
head_dim: 64
max_seq_len: 1024
num_layers: 24
d_model: 1024
d_ffn: 4096
untie_embed: false
reset_position_ids: true
reset_attention_mask: true
precision: bf16

vocab_name_or_path: gpt2
tokenizer_type: bbpe
