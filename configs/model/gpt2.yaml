num_attention_heads: 12
num_attention_groups: 12
attention_head_size: 64
head_dim: 64
max_seq_len: 1024
num_layers: 12
d_model: 768
d_ffn: 3072
untie_embed: false
reset_position_ids: false
reset_attention_mask: false
precision: bfloat16
dropout_embd: 0.0
dropout_attn: 0.0
dropout_mlp: 0.0

vocab_name_or_path: gpt2
tokenizer_type: bbpe
